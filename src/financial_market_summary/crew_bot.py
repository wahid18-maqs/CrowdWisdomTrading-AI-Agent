import logging
import time
import re
import requests
from typing import Any, Dict, List
from crewai import Agent, Crew, Process, Task
from datetime import datetime, timedelta
import json
from .agents import FinancialAgents
from .LLM_config import apply_rate_limiting
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class FinancialMarketCrew:
    """
    Enhanced CrewAI implementation with comprehensive validation for accuracy,
    real article verification, web source search, and verified image integration.
    """

    def __init__(self):
        self.agents_factory = FinancialAgents()
        self.execution_results: Dict[str, Any] = {}

    def _validate_summary(self, summary: str, original_news: str) -> Dict[str, Any]:
        """
        Comprehensive validation to ensure real articles and accurate summaries.
        """
        logger.info("--- Validating Summary for Real Content & Accuracy ---")
        
        validation_results = {
            "has_reliable_sources": self._check_sources(original_news),
            "articles_are_real": self._verify_real_articles(original_news),
            "stocks_verified": self._verify_stock_symbols(summary, original_news),
            "numbers_accurate": self._verify_numbers(summary, original_news),
            "content_fresh": self._check_freshness(original_news),
            "no_hallucination": self._check_hallucination(summary, original_news),
            "confidence_score": 0
        }
        
        # Calculate confidence score with correct keys
        weights = {
            "has_reliable_sources": 20,
            "articles_are_real": 25,
            "stocks_verified": 20,
            "numbers_accurate": 20,
            "content_fresh": 10,
            "no_hallucination": 5
        }
        
        validation_results["confidence_score"] = sum(
            weights[key] for key, passed in validation_results.items() 
            if key in weights and passed
        )
        
        logger.info(f"Validation Results: {validation_results}")
        
        if validation_results["confidence_score"] < 60:
            logger.error(f"VALIDATION FAILED: Confidence score {validation_results['confidence_score']}/100 too low")
            
        return validation_results

    def _verify_real_articles(self, original_news: str) -> bool:
        """Enhanced verification with expanded domain list."""
        urls = re.findall(r'https?://[^\s]+', original_news)
        
        if not urls:
            logger.warning("No URLs found in news content")
            return False
        
        # EXPANDED TRUSTED DOMAINS LIST
        trusted_domains = [
            # Original domains
            'yahoo.com', 'marketwatch.com', 'investing.com', 'benzinga.com', 'cnbc.com',
            
            # Additional major financial news sites
            'reuters.com', 'bloomberg.com', 'nasdaq.com', 'seekingalpha.com', 
            'fool.com', 'thestreet.com', 'wsj.com', 'ft.com', 'finance.yahoo.com',
            'morningstar.com', 'zacks.com', 'financialpost.com', 'barrons.com'
        ]
        
        verified_count = 0
        total_checked = 0
        
        for url in urls[:5]:  # Check up to 5 URLs
            total_checked += 1
            try:
                # Extract domain more safely
                if '://' in url:
                    domain = url.split('/')[2].lower()
                else:
                    continue
                    
                # Remove www. prefix if present
                if domain.startswith('www.'):
                    domain = domain[4:]
                
                # Check if any trusted domain is in the URL
                for trusted in trusted_domains:
                    if trusted in domain:
                        verified_count += 1
                        logger.info(f"✅ Verified trusted domain: {domain}")
                        break
                else:
                    logger.debug(f"❌ Domain not in trusted list: {domain}")
                    
            except Exception as e:
                logger.debug(f"Error checking URL {url}: {e}")
                continue
        
        success_rate = (verified_count / total_checked) if total_checked > 0 else 0
        is_verified = verified_count >= 1  # Need at least 1 verified article
        
        logger.info(f"🔍 Article verification: {verified_count}/{total_checked} verified ({success_rate:.1%}), Result: {is_verified}")
        
        return is_verified

    def _check_sources(self, original_news: str) -> bool:
        """Check if news comes from reliable sources."""
        # EXPANDED to match the verification domains
        reliable_sources = [
            'yahoo', 'marketwatch', 'investing.com', 'benzinga', 'cnbc',
            'reuters', 'bloomberg', 'nasdaq', 'seekingalpha', 'fool.com',
            'thestreet', 'wsj', 'ft.com', 'morningstar', 'zacks'
        ]
        return any(source in original_news.lower() for source in reliable_sources)

    def _verify_stock_symbols(self, summary: str, original_news: str) -> bool:
        """Verify stock symbols in summary exist in original news."""
        summary_stocks = set(re.findall(r'\b[A-Z]{2,5}\b', summary))
        news_stocks = set(re.findall(r'\b[A-Z]{2,5}\b', original_news))
        
        common_words = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'HER', 'WAS', 'ONE', 'OUR', 'HAD', 'HAS'}
        summary_stocks = summary_stocks - common_words
        
        return summary_stocks.issubset(news_stocks) if summary_stocks else True

    def _verify_numbers(self, summary: str, original_news: str) -> bool:
        """Verify percentages and numbers in summary match original news."""
        summary_numbers = set(re.findall(r'\d+\.?\d*%', summary))
        news_numbers = set(re.findall(r'\d+\.?\d*%', original_news))
        
        return summary_numbers.issubset(news_numbers) if summary_numbers else True

    def _check_freshness(self, original_news: str) -> bool:
        """Check if news is recent."""
        time_patterns = [
            r'(\d{4}-\d{2}-\d{2})',
            r'(\d{1,2} hours? ago)',
            r'(today|yesterday)',
        ]
        
        for pattern in time_patterns:
            if re.search(pattern, original_news, re.IGNORECASE):
                return True
        
        return True

    def _check_hallucination(self, summary: str, original_news: str) -> bool:
        """Check for hallucination by word overlap."""
        summary_words = set(summary.lower().split())
        news_words = set(original_news.lower().split())
        
        common_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had'}
        summary_words = summary_words - common_words
        news_words = news_words - common_words
        
        if not summary_words:
            return False
            
        overlap = len(summary_words.intersection(news_words)) / len(summary_words)
        return overlap > 0.3

    # NEW WEB SOURCE VERIFICATION METHODS
    def _run_web_source_search_phase(self, summary_title: str, key_themes: List[str], mentioned_stocks: List[str]) -> Dict[str, Any]:
        """Search the web to find the best source article matching the summary."""
        logger.info(f"--- Phase 2.1: Web Search for Source Matching '{summary_title[:50]}...' ---")
        source_agent = self.agents_factory.web_searching_source_agent()
        
        # Build search query from summary elements
        search_query = self._build_source_search_query(summary_title, key_themes, mentioned_stocks)
        
        source_task = Task(
            description=f"""Search the web to find the best source article for this financial summary:

            SUMMARY TITLE: "{summary_title}"
            KEY THEMES: {key_themes}
            MENTIONED STOCKS: {mentioned_stocks}
            
            SEARCH STRATEGY:
            1. Use tavily_financial_search to find recent articles matching the summary
            2. Search query: "{search_query}"
            3. Focus on last 24 hours for maximum relevance
            4. Target trusted financial news sources
            5. VERIFY each URL works (no 404 errors)
            
            EVALUATION CRITERIA:
            1. TITLE RELEVANCE (40%): Article title closely matches summary title
            2. CONTENT ALIGNMENT (30%): Article covers same themes/events as summary
            3. RECENCY (20%): Published within last 24 hours
            4. SOURCE AUTHORITY (10%): From reputable financial news outlet
            
            TRUSTED SOURCES: Yahoo Finance, CNBC, Reuters, MarketWatch, Bloomberg, WSJ, Investing.com
            
            RETURN the single best matching article as JSON:
            {{
                "main_source": {{
                    "title": "Article title that best matches summary",
                    "url": "https://...",
                    "source": "Yahoo Finance",
                    "published": "Recent",
                    "relevance_score": 95,
                    "match_explanation": "Perfect match - covers same market records and Fed signals",
                    "url_verified": true
                }},
                "search_query_used": "{search_query}",
                "total_results_found": 8,
                "confidence_score": 95
            }}""",
            expected_output="JSON with the best web-searched source article that matches the summary title.",
            agent=source_agent
        )
        
        result = self._run_task_with_retry([source_agent], source_task)
        
        try:
            # Parse JSON result
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                source_data = json.loads(json_match.group())
                
                # Verify the URL exists and is accessible
                source_data = self._verify_source_url(source_data)
                
                logger.info(f"Found web source: {source_data.get('main_source', {}).get('title', 'N/A')[:60]}... (Score: {source_data.get('confidence_score', 0)}) [URL Verified: {source_data.get('main_source', {}).get('url_verified', False)}]")
                return source_data
            else:
                logger.warning("No JSON found in web search result")
                return self._web_search_fallback(summary_title, key_themes, mentioned_stocks)
                
        except Exception as e:
            logger.warning(f"Web source search parsing failed: {e}")
            return self._web_search_fallback(summary_title, key_themes, mentioned_stocks)

    def _verify_source_url(self, source_data: Dict[str, Any]) -> Dict[str, Any]:
        """Verify that the source URL exists and is accessible (no 404)."""
        main_source = source_data.get("main_source", {})
        url = main_source.get("url", "")
        
        if not url or not url.startswith(("http://", "https://")):
            logger.warning(f"Invalid URL format: {url}")
            return self._handle_invalid_url(source_data)
        
        try:
            # Set reasonable timeout and headers to mimic browser request
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            # Make HEAD request first (faster than GET)
            response = requests.head(url, headers=headers, timeout=10, allow_redirects=True)
            
            if response.status_code == 200:
                # URL is accessible
                main_source["url_verified"] = True
                main_source["verification_status"] = "accessible"
                logger.info(f"✅ URL verified accessible: {url}")
                return source_data
                
            elif response.status_code in [301, 302, 303, 307, 308]:
                # Handle redirects
                final_url = response.url if hasattr(response, 'url') else url
                main_source["url"] = final_url
                main_source["url_verified"] = True
                main_source["verification_status"] = f"redirected_to_{final_url}"
                logger.info(f"✅ URL verified (redirected): {url} → {final_url}")
                return source_data
                
            elif response.status_code == 404:
                logger.warning(f"❌ URL returns 404: {url}")
                return self._handle_404_url(source_data)
                
            else:
                logger.warning(f"⚠️ URL returns status {response.status_code}: {url}")
                return self._handle_problematic_url(source_data, response.status_code)
                
        except requests.exceptions.Timeout:
            logger.warning(f"⏱️ URL verification timeout: {url}")
            return self._handle_timeout_url(source_data)
            
        except requests.exceptions.ConnectionError:
            logger.warning(f"🔌 URL connection error: {url}")
            return self._handle_connection_error_url(source_data)
            
        except Exception as e:
            logger.warning(f"❌ URL verification failed: {url} - {str(e)}")
            return self._handle_verification_error(source_data, str(e))

    def _handle_404_url(self, source_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle URLs that return 404 errors."""
        main_source = source_data.get("main_source", {})
        original_url = main_source.get("url", "")
        
        # Try to get domain homepage as fallback
        try:
            parsed = urlparse(original_url)
            domain_fallback = f"{parsed.scheme}://{parsed.netloc}"
            
            # Update to use domain homepage
            main_source["url"] = domain_fallback
            main_source["url_verified"] = False
            main_source["verification_status"] = "404_fallback_to_homepage"
            main_source["match_explanation"] = f"Original article not found, using {parsed.netloc} homepage"
            
            # Reduce confidence score due to 404
            source_data["confidence_score"] = max(30, source_data.get("confidence_score", 50) - 40)
            
            logger.info(f"🔄 404 fallback: {original_url} → {domain_fallback}")
            return source_data
            
        except Exception as e:
            logger.warning(f"Failed to create 404 fallback: {e}")
            return self._web_search_fallback("", [], [])

    def _handle_invalid_url(self, source_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle invalid URL formats."""
        main_source = source_data.get("main_source", {})
        main_source["url"] = "https://finance.yahoo.com"
        main_source["source"] = "Yahoo Finance"
        main_source["url_verified"] = True
        main_source["verification_status"] = "invalid_url_fallback"
        main_source["match_explanation"] = "Invalid URL format, using Yahoo Finance homepage"
        source_data["confidence_score"] = 40
        return source_data

    def _handle_problematic_url(self, source_data: Dict[str, Any], status_code: int) -> Dict[str, Any]:
        """Handle URLs with problematic status codes."""
        main_source = source_data.get("main_source", {})
        main_source["url_verified"] = False
        main_source["verification_status"] = f"status_{status_code}"
        
        # If it's a client error (4xx), treat more seriously
        if 400 <= status_code < 500:
            source_data["confidence_score"] = max(25, source_data.get("confidence_score", 50) - 30)
            main_source["match_explanation"] += f" (Warning: HTTP {status_code})"
        else:
            # Server error (5xx) or other - less severe reduction
            source_data["confidence_score"] = max(40, source_data.get("confidence_score", 50) - 20)
        
        return source_data

    def _handle_timeout_url(self, source_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle URL verification timeouts."""
        main_source = source_data.get("main_source", {})
        main_source["url_verified"] = False
        main_source["verification_status"] = "timeout"
        main_source["match_explanation"] += " (Verification timeout)"
        # Don't reduce confidence too much for timeouts
        source_data["confidence_score"] = max(60, source_data.get("confidence_score", 70) - 10)
        return source_data

    def _handle_connection_error_url(self, source_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle URL connection errors."""
        main_source = source_data.get("main_source", {})
        main_source["url_verified"] = False
        main_source["verification_status"] = "connection_error"
        source_data["confidence_score"] = max(35, source_data.get("confidence_score", 50) - 25)
        return self._handle_404_url(source_data)  # Use same fallback as 404

    def _handle_verification_error(self, source_data: Dict[str, Any], error_msg: str) -> Dict[str, Any]:
        """Handle general verification errors."""
        main_source = source_data.get("main_source", {})
        main_source["url_verified"] = False
        main_source["verification_status"] = f"error_{error_msg[:20]}"
        source_data["confidence_score"] = max(45, source_data.get("confidence_score", 60) - 15)
        return source_data

    def _build_source_search_query(self, title: str, themes: List[str], stocks: List[str]) -> str:
        """Build optimized search query for finding source articles."""
        # Extract key terms from title
        title_words = title.lower().split()
        
        # Key financial terms that should be in search
        important_terms = []
        for word in title_words:
            if word in ['stocks', 'market', 'earnings', 'fed', 'record', 'high', 'surge', 'rally', 'gains', 'nasdaq', 'dow', 's&p']:
                important_terms.append(word)
        
        # Add theme-specific terms
        theme_terms = {
            'earnings': ['earnings', 'quarterly results'],
            'fed_policy': ['federal reserve', 'rate cut'],
            'market_records': ['record high', 'market surge'],
            'technology': ['tech stocks', 'technology']
        }
        
        for theme in themes:
            if theme in theme_terms:
                important_terms.extend(theme_terms[theme])
        
        # Add top mentioned stocks
        stock_terms = stocks[:3] if stocks else []
        
        # Combine all terms intelligently
        query_parts = important_terms[:4] + stock_terms[:2]  # Limit to prevent over-long queries
        
        return ' '.join(query_parts)

    def _web_search_fallback(self, title: str, themes: List[str], stocks: List[str]) -> Dict[str, Any]:
        """Fallback when web search agent fails."""
        # Create a simpler search query and try manual search
        simple_query = ' '.join(title.split()[:6])  # First 6 words of title
        
        return {
            "main_source": {
                "title": f"Financial News: {title[:50]}...",
                "url": "https://finance.yahoo.com",
                "source": "Yahoo Finance",
                "published": "Recent",
                "relevance_score": 50,
                "match_explanation": "Fallback source - web search failed",
                "url_verified": True,
                "verification_status": "fallback_verified"
            },
            "search_query_used": simple_query,
            "total_results_found": 0,
            "confidence_score": 50
        }

    def _extract_summary_title(self, summary: str) -> str:
        """Extract title from summary."""
        lines = summary.strip().split('\n')
        for line in lines:
            if line.strip() and not line.startswith('•') and not line.startswith('-'):
                return line.strip().replace('**', '').replace('#', '').strip()
        return "Market Update"

    def _extract_title_themes(self, title: str) -> List[str]:
        """Extract themes from title."""
        themes = []
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['earnings', 'quarterly', 'results']):
            themes.append('earnings')
        if any(word in title_lower for word in ['fed', 'federal', 'rate']):
            themes.append('fed_policy')
        if any(word in title_lower for word in ['record', 'high', 'surge', 'rally']):
            themes.append('market_records')
        if any(word in title_lower for word in ['tech', 'technology', 'nvidia', 'intel']):
            themes.append('technology')
            
        return themes

    def _extract_mentioned_stocks(self, summary: str) -> List[str]:
        """Extract stock symbols mentioned in summary."""
        stocks = re.findall(r'\b([A-Z]{2,5})\b', summary)
        major_stocks = {'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'TSLA', 'NVDA', 'META', 'FDX', 'INTC'}
        return [stock for stock in stocks if stock in major_stocks]

    def _run_enhanced_image_search_phase(self, summary_title: str, key_themes: List[str], mentioned_stocks: List[str], content: str) -> Dict[str, Any]:
        """Search for and verify relevant financial images"""
        logger.info(f"--- Phase 2.2: Enhanced Image Search for '{summary_title[:50]}...' ---")
        
        try:
            # Use the enhanced image agent
            image_search_result = self.agents_factory.enhanced_image_agent.run_image_search_phase(
                summary_title, key_themes, mentioned_stocks, content
            )
            
            # Log results
            verified_count = image_search_result.get('verified_count', 0)
            total_count = image_search_result.get('total_images_found', 0)
            confidence = image_search_result.get('confidence_score', 0)
            
            logger.info(f"Image search completed: {verified_count}/{total_count} verified (Confidence: {confidence}/100)")
            
            return image_search_result
            
        except Exception as e:
            logger.warning(f"Enhanced image search failed: {e}")
            return self._image_search_fallback(summary_title, mentioned_stocks)

    def _select_best_verified_image(self, image_search_data: Dict[str, Any]) -> Dict[str, Any]:
        """Select the best verified image from search results"""
        try:
            best_image = self.agents_factory.enhanced_image_agent.select_best_image(image_search_data)
            
            if best_image:
                logger.info(f"Selected verified image: {best_image.get('title', 'Unknown')[:50]}... (Verified: {best_image.get('url_verified', False)})")
                return best_image
            else:
                logger.warning("No suitable images found, using fallback")
                return self._get_fallback_image()
                
        except Exception as e:
            logger.warning(f"Image selection failed: {e}")
            return self._get_fallback_image()

    def _image_search_fallback(self, title: str, stocks: List[str]) -> Dict[str, Any]:
        """Fallback when image search fails"""
        fallback_images = []
        
        # S&P 500 chart (always reliable)
        fallback_images.append({
            "url": "https://chart.yahoo.com/z?s=%5EGSPC&t=1d&q=l&l=on&z=s&p=s",
            "title": "S&P 500 Market Chart",
            "source": "Yahoo Finance",
            "type": "market_index",
            "relevance_score": 70,
            "url_verified": True,
            "verification_status": "fallback_reliable"
        })
        
        # Add stock-specific chart if stocks mentioned
        if stocks:
            primary_stock = stocks[0]
            fallback_images.append({
                "url": f"https://chart.yahoo.com/z?s={primary_stock}&t=1d&q=l&l=on&z=s&p=s",
                "title": f"{primary_stock} Stock Chart",
                "source": "Yahoo Finance",
                "type": "stock_chart",
                "stock_symbol": primary_stock,
                "relevance_score": 75,
                "url_verified": True,
                "verification_status": "fallback_reliable"
            })
        
        return {
            "verified_images": fallback_images,
            "search_strategy_used": "Fallback strategy - reliable charts",
            "total_images_found": len(fallback_images),
            "verified_count": len(fallback_images),
            "confidence_score": 60,
            "verification_rate": 100,
            "fallback_used": True
        }

    def _get_fallback_image(self) -> Dict[str, Any]:
        """Get single fallback image"""
        return {
            "url": "https://chart.yahoo.com/z?s=%5EGSPC&t=1d&q=l&l=on&z=s&p=s",
            "title": "S&P 500 Market Chart",
            "source": "Yahoo Finance",
            "type": "fallback_market",
            "relevance_score": 60,
            "url_verified": True,
            "verification_status": "fallback_reliable",
            "fallback_used": True
        }
        """Extract stock symbols mentioned in summary."""
        stocks = re.findall(r'\b([A-Z]{2,5})\b', summary)
        major_stocks = {'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'TSLA', 'NVDA', 'META', 'FDX', 'INTC'}
        return [stock for stock in stocks if stock in major_stocks]

    def _run_task_with_retry(self, agents: List[Agent], task: Task, max_retries: int = 3) -> str:
        """Run task with enhanced error handling for empty LLM responses."""
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    wait_time = 5 * (attempt + 1)
                    logger.info(f"Retry attempt {attempt + 1}, waiting {wait_time} seconds...")
                    time.sleep(wait_time)

                crew = Crew(
                    agents=agents,
                    tasks=[task],
                    process=Process.sequential,
                    verbose=True
                )
                
                result = crew.kickoff()
                
                # Check if result is empty or None
                if not result or str(result).strip() == "" or str(result).lower() == "none":
                    logger.warning(f"Empty response on attempt {attempt + 1}, retrying...")
                    if attempt < max_retries - 1:
                        continue
                    else:
                        return "Error: LLM returned empty response after all retries"
                
                return str(result)

            except Exception as e:
                error_msg = str(e)
                logger.error(f"Task execution error on attempt {attempt + 1}: {error_msg}")
                
                if any(limit_keyword in error_msg.lower() for limit_keyword in ["429", "quota", "rate limit", "quota exceeded"]):
                    logger.warning(f"Rate limit hit on attempt {attempt + 1}")
                    if attempt < max_retries - 1:
                        time.sleep(10 * (attempt + 1))  # Longer wait for rate limits
                        continue
                elif "empty" in error_msg.lower() or "none" in error_msg.lower():
                    logger.warning(f"Empty response error on attempt {attempt + 1}")
                    if attempt < max_retries - 1:
                        time.sleep(5)
                        continue
                else:
                    logger.error(f"Non-recoverable error: {error_msg}")
                    if attempt < max_retries - 1:
                        continue

                return f"Error after {max_retries} attempts: {error_msg}"
        
        return "Error: Max retries exceeded with empty responses"

    def run_complete_workflow(self) -> Dict[str, Any]:
        """Execute workflow with web-only source search."""
        try:
            logger.info("--- Starting Workflow with Web Source Search and Validation ---")

            # Phase 1: Search (original news gathering)
            search_result = self._run_search_phase()
            if "Error" in search_result:
                return {"status": "failed", "error": f"Search phase failed: {search_result}"}
            self.execution_results["search"] = search_result

            # Phase 2: Summary
            summary_result = self._run_summary_phase(search_result)
            if "Error" in summary_result:
                return {"status": "failed", "error": f"Summary phase failed: {summary_result}"}
            
            # Extract summary metadata for source search
            summary_title = self._extract_summary_title(summary_result)
            key_themes = self._extract_title_themes(summary_title)
            mentioned_stocks = self._extract_mentioned_stocks(summary_result)
            
            # Phase 2.1: Web Source Search
            web_source_data = self._run_web_source_search_phase(summary_title, key_themes, mentioned_stocks)
            self.execution_results["web_source"] = web_source_data
            
            # Phase 2.2: Enhanced Image Search (NEW)
            image_search_data = self._run_enhanced_image_search_phase(summary_title, key_themes, mentioned_stocks, summary_result)
            self.execution_results["image_search"] = image_search_data
            
            # Use web source as the selected source
            selected_source = web_source_data.copy()
            selected_source["method"] = "web_search"
            self.execution_results["selected_source"] = selected_source
            
            # Select best verified image
            best_image = self._select_best_verified_image(image_search_data)
            self.execution_results["selected_image"] = best_image
            
            # Phase 2.5: Validation
            validation_results = self._validate_summary(summary_result, search_result)
            self.execution_results["validation"] = validation_results
            
            if validation_results["confidence_score"] < 60:
                return {
                    "status": "failed", 
                    "error": f"Validation failed: Confidence score {validation_results['confidence_score']}/100",
                    "validation_details": validation_results
                }
            
            self.execution_results["summary"] = summary_result

            # Phase 3: Formatting with verified source and image
            formatted_result = self._run_formatting_phase(summary_result, selected_source, best_image)
            if "Error" in formatted_result:
                return {"status": "failed", "error": f"Formatting phase failed: {formatted_result}"}
            self.execution_results["formatted_summary"] = formatted_result

            # Phase 4: Translation
            translations = self._run_translation_phase(formatted_result)
            self.execution_results["translations"] = translations

            # Phase 5: Enhanced Sending with Web Source
            send_results = self._run_enhanced_sending_phase(formatted_result, translations, selected_source)
            self.execution_results["send_results"] = send_results

            logger.info("--- Web Source Search Workflow Completed Successfully ---")
            return {
                "status": "success",
                "results": self.execution_results,
                "execution_time": datetime.now().isoformat(),
                "summary": {
                    "english_summary_excerpt": formatted_result[:200] + "..." if len(formatted_result) > 200 else formatted_result,
                    "translations_completed": len([k for k, v in translations.items() if not isinstance(v, str) or "failed" not in v.lower()]),
                    "sends_completed": len([k for k, v in send_results.items() if "successfully" in v.lower()]),
                    "confidence_score": validation_results["confidence_score"],
                    "validation_passed": validation_results["confidence_score"] >= 60,
                    "real_articles_verified": validation_results["articles_are_real"],
                    "source_method": "web_search",
                    "source_confidence": selected_source.get("confidence_score", 0),
                    "source_title": selected_source.get("main_source", {}).get("title", "")[:60] + "...",
                    "source_url_verified": selected_source.get("main_source", {}).get("url_verified", False),
                    "images_found": image_search_data.get("total_images_found", 0),
                    "images_verified": image_search_data.get("verified_count", 0),
                    "image_confidence": image_search_data.get("confidence_score", 0),
                    "best_image_verified": best_image.get("url_verified", False) if best_image else False,
                    "best_image_title": best_image.get("title", "No image")[:50] + "..." if best_image else "No image",
                },
            }

        except Exception as e:
            error_msg = f"Web source workflow failed: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return {"status": "failed", "error": error_msg}

    def _run_search_phase(self) -> str:
        """Search phase with 1-hour enforcement."""
        logger.info("--- Phase 1: Searching for Real-Time News (Last 1 Hour) ---")
        search_agent = self.agents_factory.search_agent()
        search_task = Task(
            description="""Search for the latest US financial news from EXACTLY the past 1 hour.
            
            CRITICAL REQUIREMENTS:
            - Must be from last 1 hour only for real-time relevance
            - Include actual article URLs that are accessible
            - Focus on major stock movements, earnings, economic data
            - Ensure sources are from reliable financial news outlets
            
            Use tavily_financial_search tool with hours_back=1 enforced.""",
            expected_output="Structured financial news from last 1 hour with verified URLs and reliable sources.",
            agent=search_agent
        )
        return self._run_task_with_retry([search_agent], search_task)

    @apply_rate_limiting("gemini")
    def _run_summary_phase(self, search_result: str) -> str:
        """Summary phase with pre-validation."""
        logger.info("--- Phase 2: Creating Accurate Summary ---")
        
        # Validate search results before attempting summary
        if not search_result or len(search_result.strip()) < 50:
            logger.error("Search results are empty or too short for summarization")
            return "Error: No sufficient news data found to create summary"
        
        logger.info(f"Search result length: {len(search_result)} characters")
        
        summary_agent = self.agents_factory.summary_agent()
        summary_task = Task(
            description=f"Create a simple financial summary from this news: {search_result[:1000]}...\n\nInclude: Title, Key Points, Market Implications. Keep under 300 words.",
            expected_output="Financial summary with title, key points, and market implications.",
            agent=summary_agent
        )
        return self._run_task_with_retry([summary_agent], summary_task)

    @apply_rate_limiting("gemini")
    def _run_formatting_phase(self, summary_result: str, selected_source: Dict[str, Any], best_image: Dict[str, Any]) -> str:
        """Formatting phase with verified image integration and source."""
        logger.info("--- Phase 3: Formatting with Verified Images and Source ---")
        formatting_agent = self.agents_factory.formatting_agent()
        
        # Extract source information
        main_source = selected_source.get("main_source", {})
        source_title = main_source.get("title", "Financial News")
        source_url = main_source.get("url", "https://finance.yahoo.com")
        source_name = main_source.get("source", "Yahoo Finance")
        url_verified = main_source.get("url_verified", False)
        verification_status = "✅" if url_verified else "⚠️"
        
        # Extract image information
        image_info = ""
        if best_image:
            image_title = best_image.get("title", "Financial Chart")
            image_url = best_image.get("url", "")
            image_verified = best_image.get("url_verified", False)
            image_source = best_image.get("source", "Yahoo Finance")
            image_verification = "✅" if image_verified else "⚠️"
            
            image_info = f"""
            VERIFIED IMAGE INFORMATION:
            - Title: {image_title}
            - URL: {image_url}
            - Source: {image_source}
            - Verified: {image_verified} {image_verification}
            - Type: {best_image.get('type', 'chart')}
            """
        
        formatting_task = Task(
            description=f"""Format the verified summary for Telegram delivery with source and image information: {summary_result}

            VERIFIED SOURCE INFORMATION:
            - Title: {source_title}
            - URL: {source_url}
            - Source: {source_name}
            - Verified: {url_verified} {verification_status}
            
            {image_info}
            
            Requirements:
            1. Include the verified source as a clickable link: [{source_title} - {source_name}]({source_url}) {verification_status}
            2. If verified image available, mention it will be included with the message
            3. Use the enhanced_financial_image_finder tool for additional backup images if needed
            4. Ensure images are from trusted sources (Yahoo Finance, TradingView, etc.)
            5. Format for clean Telegram delivery with proper HTML/Markdown
            6. Structure: Title -> Verified Source -> Key Points -> Market Implications -> Charts Note
            7. Add confidence indicator if URL verification had issues
            8. Note that verified financial chart will be attached to message

            The telegram_sender will automatically include the verified image.""",
            expected_output="Clean, formatted summary optimized for Telegram with verified source and image context.",
            agent=formatting_agent
        )
        return self._run_task_with_retry([formatting_agent], formatting_task)

    def _run_translation_phase(self, formatted_content: str) -> Dict[str, str]:
        """Translation phase maintaining accuracy."""
        logger.info("--- Phase 4: Accurate Translation ---")
        translation_agent = self.agents_factory.translation_agent()
        translations = {}
        languages = ["arabic", "hindi", "hebrew"]

        for lang in languages:
            logger.info(f"Translating to {lang} with accuracy preservation...")
            translation_task = Task(
                description=f"""Translate to {lang} while preserving accuracy: {formatted_content}

                CRITICAL REQUIREMENTS:
                - Keep ALL stock symbols unchanged (AAPL, MSFT, etc.)
                - Preserve ALL numbers, percentages, currency values exactly
                - Maintain markdown/HTML formatting and source links
                - Use professional financial terminology
                - Do NOT add any new information during translation
                - Keep source verification indicators (✅ ⚠️)

                The translation will be validated for accuracy.""",
                expected_output=f"Accurate translation in {lang} with preserved financial data and source links.",
                agent=translation_agent
            )
            translations[lang] = self._run_task_with_retry([translation_agent], translation_task)

        return translations

    def _run_enhanced_sending_phase(self, formatted_content: str, translations: Dict[str, str], selected_source: Dict[str, Any]) -> Dict[str, str]:
        """Enhanced sending phase with verified content, images, and source information."""
        logger.info("--- Phase 5: Sending Verified Content with Source to Telegram ---")
        send_agent = self.agents_factory.send_agent()
        send_results = {}

        # Add validation context to message if confidence is low
        validation = self.execution_results.get("validation", {})
        confidence_score = validation.get("confidence_score", 0)
        source_confidence = selected_source.get("confidence_score", 0)
        url_verified = selected_source.get("main_source", {}).get("url_verified", False)
        
        # Create confidence footer
        confidence_footer = f"\n\n**📊 Confidence: {confidence_score}/100**"
        if source_confidence < 80:
            confidence_footer += f" | **🔗 Source: {source_confidence}/100**"
        if url_verified:
            confidence_footer += " | **🔗 URL Verified** ✅"
        else:
            confidence_footer += " | **🔗 Fallback Source** ⚠️"
        
        enhanced_content = formatted_content + confidence_footer

        # Send English summary with verified source and images
        english_send_task = Task(
            description=f"""Send verified English financial summary to Telegram: {enhanced_content}

            SOURCE VERIFICATION STATUS:
            - URL Verified: {url_verified}
            - Source Confidence: {source_confidence}/100
            - Validation Confidence: {confidence_score}/100
            
            The enhanced telegram_sender will:
            - Verify image legitimacy and contextual relevance
            - Use only trusted financial image sources
            - Send text-only if no verified images available
            - Include source verification status in message
            - Ensure clickable source links work properly

            Use telegram_sender tool with language='english'.""",
            expected_output="Confirmation of successful delivery with verified image and source status.",
            agent=send_agent
        )
        send_results["english"] = self._run_task_with_retry([send_agent], english_send_task)

        # Send translations with source verification
        for lang, content in translations.items():
            if "Error" not in content:
                enhanced_translated_content = content + confidence_footer
                lang_send_task = Task(
                    description=f"""Send verified {lang} financial summary: {enhanced_translated_content}

                    Same verification process as English version with source verification status.""",
                    expected_output=f"Confirmation of {lang} message delivery with verification status.",
                    agent=send_agent
                )
                send_results[lang] = self._run_task_with_retry([send_agent], lang_send_task)
            else:
                send_results[lang] = f"Skipped {lang} due to translation failure."

        return send_results